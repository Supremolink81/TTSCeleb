{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install requirements**"
      ],
      "metadata": {
        "id": "YLRuc86mSDuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install encodec git+https://github.com/suno-ai/bark.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuul9LWrSDKu",
        "outputId": "d5c72a46-5b36-4353-e4c1-dc7baaf921c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/suno-ai/bark.git\n",
            "  Cloning https://github.com/suno-ai/bark.git to /tmp/pip-req-build-sgozlela\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/suno-ai/bark.git /tmp/pip-req-build-sgozlela\n",
            "  Resolved https://github.com/suno-ai/bark.git to commit 56b0ba13f7c281cbffa07ea9abf7b30273a60b6a\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting encodec\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from encodec) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from encodec) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from encodec) (2.0.2+cu118)\n",
            "Collecting einops (from encodec)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3 (from suno-bark==0.0.1a0)\n",
            "  Downloading boto3-1.28.8-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting funcy (from suno-bark==0.0.1a0)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Collecting huggingface-hub>=0.14.1 (from suno-bark==0.0.1a0)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (1.10.1)\n",
            "Collecting tokenizers (from suno-bark==0.0.1a0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (4.65.0)\n",
            "Collecting transformers (from suno-bark==0.0.1a0)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (23.1)\n",
            "Collecting botocore<1.32.0,>=1.31.8 (from boto3->suno-bark==0.0.1a0)\n",
            "  Downloading botocore-1.31.8-py3-none-any.whl (11.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->suno-bark==0.0.1a0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->suno-bark==0.0.1a0)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->encodec) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->encodec) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->suno-bark==0.0.1a0) (2022.10.31)\n",
            "Collecting safetensors>=0.3.1 (from transformers->suno-bark==0.0.1a0)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.8->boto3->suno-bark==0.0.1a0) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.8->boto3->suno-bark==0.0.1a0) (1.26.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->encodec) (2.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->encodec) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.8->boto3->suno-bark==0.0.1a0) (1.16.0)\n",
            "Building wheels for collected packages: encodec, suno-bark\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=b2bc89100ad3a53c9702abd1480bbc2aecd3bb2c29be26109059b080d85a66a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "  Building wheel for suno-bark (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for suno-bark: filename=suno_bark-0.0.1a0-py3-none-any.whl size=2567434 sha256=67c3b5622539700470ca0d7cbbd15b343cf11db39cf3016f232c60d19c8a2d52\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_u_g8lg5/wheels/e6/6d/c2/107ed849afe600f905bb4049a026df3c7c5aa75d86c2721ec7\n",
            "Successfully built encodec suno-bark\n",
            "Installing collected packages: tokenizers, safetensors, funcy, jmespath, einops, huggingface-hub, botocore, transformers, s3transfer, boto3, encodec, suno-bark\n",
            "Successfully installed boto3-1.28.8 botocore-1.31.8 einops-0.6.1 encodec-0.1.1 funcy-2.0 huggingface-hub-0.16.4 jmespath-1.0.1 s3transfer-0.6.1 safetensors-0.3.1 suno-bark-0.0.1a0 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BEATs setup**\n",
        "\n",
        "(Not my code, all credit for the code from here to the History Prompt section goes to the team at Microsoft Research Asia who coded it)."
      ],
      "metadata": {
        "id": "If1-qYLP1rcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "modules.py"
      ],
      "metadata": {
        "id": "R8_CB2CL2yxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# BEATs: Audio Pre-Training with Acoustic Tokenizers (https://arxiv.org/abs/2212.09058)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/beats\n",
        "# Copyright (c) 2022 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import warnings\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GradMultiply(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, scale):\n",
        "        ctx.scale = scale\n",
        "        res = x.new(x)\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad):\n",
        "        return grad * ctx.scale, None\n",
        "\n",
        "\n",
        "class SamePad(nn.Module):\n",
        "    def __init__(self, kernel_size, causal=False):\n",
        "        super().__init__()\n",
        "        if causal:\n",
        "            self.remove = kernel_size - 1\n",
        "        else:\n",
        "            self.remove = 1 if kernel_size % 2 == 0 else 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.remove > 0:\n",
        "            x = x[:, :, : -self.remove]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Swish, self).__init__()\n",
        "        self.act = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.act(x)\n",
        "\n",
        "\n",
        "class GLU_Linear(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, glu_type=\"sigmoid\", bias_in_glu=True):\n",
        "        super(GLU_Linear, self).__init__()\n",
        "\n",
        "        self.glu_type = glu_type\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if glu_type == \"sigmoid\":\n",
        "            self.glu_act = torch.nn.Sigmoid()\n",
        "        elif glu_type == \"swish\":\n",
        "            self.glu_act = Swish()\n",
        "        elif glu_type == \"relu\":\n",
        "            self.glu_act = torch.nn.ReLU()\n",
        "        elif glu_type == \"gelu\":\n",
        "            self.glu_act = torch.nn.GELU()\n",
        "\n",
        "        if bias_in_glu:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, True)\n",
        "        else:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # to be consistent with GLU_Linear, we assume the input always has the #channel (#dim) in the last dimension of the tensor, so need to switch the dimension first for 1D-Conv case\n",
        "        x = self.linear(x)\n",
        "\n",
        "        if self.glu_type == \"bilinear\":\n",
        "            x = (x[:, :, 0:self.output_dim] * x[:, :, self.output_dim:self.output_dim * 2])\n",
        "        else:\n",
        "            x = (x[:, :, 0:self.output_dim] * self.glu_act(x[:, :, self.output_dim:self.output_dim * 2]))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def gelu_accurate(x):\n",
        "    if not hasattr(gelu_accurate, \"_a\"):\n",
        "        gelu_accurate._a = math.sqrt(2 / math.pi)\n",
        "    return (\n",
        "        0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    )\n",
        "\n",
        "\n",
        "def gelu(x: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.nn.functional.gelu(x.float()).type_as(x)\n",
        "\n",
        "\n",
        "def get_activation_fn(activation: str):\n",
        "    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return gelu\n",
        "    elif activation == \"gelu_fast\":\n",
        "        warnings.warn(\n",
        "            \"--activation-fn=gelu_fast has been renamed to gelu_accurate\"\n",
        "        )\n",
        "        return gelu_accurate\n",
        "    elif activation == \"gelu_accurate\":\n",
        "        return gelu_accurate\n",
        "    elif activation == \"tanh\":\n",
        "        return torch.tanh\n",
        "    elif activation == \"linear\":\n",
        "        return lambda x: x\n",
        "    elif activation == \"glu\":\n",
        "        return lambda x: x\n",
        "    else:\n",
        "        raise RuntimeError(\"--activation-fn {} not supported\".format(activation))\n",
        "\n",
        "\n",
        "def quant_noise(module, p, block_size):\n",
        "    \"\"\"\n",
        "    Wraps modules and applies quantization noise to the weights for\n",
        "    subsequent quantization with Iterative Product Quantization as\n",
        "    described in \"Training with Quantization Noise for Extreme Model Compression\"\n",
        "\n",
        "    Args:\n",
        "        - module: nn.Module\n",
        "        - p: amount of Quantization Noise\n",
        "        - block_size: size of the blocks for subsequent quantization with iPQ\n",
        "\n",
        "    Remarks:\n",
        "        - Module weights must have the right sizes wrt the block size\n",
        "        - Only Linear, Embedding and Conv2d modules are supported for the moment\n",
        "        - For more detail on how to quantize by blocks with convolutional weights,\n",
        "          see \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\"\n",
        "        - We implement the simplest form of noise here as stated in the paper\n",
        "          which consists in randomly dropping blocks\n",
        "    \"\"\"\n",
        "\n",
        "    # if no quantization noise, don't register hook\n",
        "    if p <= 0:\n",
        "        return module\n",
        "\n",
        "    # supported modules\n",
        "    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n",
        "\n",
        "    # test whether module.weight has the right sizes wrt block_size\n",
        "    is_conv = module.weight.ndim == 4\n",
        "\n",
        "    # 2D matrix\n",
        "    if not is_conv:\n",
        "        assert (\n",
        "            module.weight.size(1) % block_size == 0\n",
        "        ), \"Input features must be a multiple of block sizes\"\n",
        "\n",
        "    # 4D matrix\n",
        "    else:\n",
        "        # 1x1 convolutions\n",
        "        if module.kernel_size == (1, 1):\n",
        "            assert (\n",
        "                module.in_channels % block_size == 0\n",
        "            ), \"Input channels must be a multiple of block sizes\"\n",
        "        # regular convolutions\n",
        "        else:\n",
        "            k = module.kernel_size[0] * module.kernel_size[1]\n",
        "            assert k % block_size == 0, \"Kernel size must be a multiple of block size\"\n",
        "\n",
        "    def _forward_pre_hook(mod, input):\n",
        "        # no noise for evaluation\n",
        "        if mod.training:\n",
        "            if not is_conv:\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_features = weight.size(1)\n",
        "                out_features = weight.size(0)\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                mask = torch.zeros(\n",
        "                    in_features // block_size * out_features, device=weight.device\n",
        "                )\n",
        "                mask.bernoulli_(p)\n",
        "                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n",
        "\n",
        "            else:\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_channels = mod.in_channels\n",
        "                out_channels = mod.out_channels\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                if mod.kernel_size == (1, 1):\n",
        "                    mask = torch.zeros(\n",
        "                        int(in_channels // block_size * out_channels),\n",
        "                        device=weight.device,\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n",
        "                else:\n",
        "                    mask = torch.zeros(\n",
        "                        weight.size(0), weight.size(1), device=weight.device\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = (\n",
        "                        mask.unsqueeze(2)\n",
        "                        .unsqueeze(3)\n",
        "                        .repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n",
        "                    )\n",
        "\n",
        "            # scale weights and apply mask\n",
        "            mask = mask.to(\n",
        "                torch.bool\n",
        "            )  # x.bool() is not currently supported in TorchScript\n",
        "            s = 1 / (1 - p)\n",
        "            mod.weight.data = s * weight.masked_fill(mask, 0)\n",
        "\n",
        "    module.register_forward_pre_hook(_forward_pre_hook)\n",
        "    return module"
      ],
      "metadata": {
        "id": "axyoKboM2ycN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "backbone.py"
      ],
      "metadata": {
        "id": "ug47TwGE14vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# BEATs: Audio Pre-Training with Acoustic Tokenizers (https://arxiv.org/abs/2212.09058)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/beats\n",
        "# Copyright (c) 2022 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import Dict, Optional, Tuple\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LayerNorm, Parameter\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.embedding_dim = args.encoder_embed_dim\n",
        "\n",
        "        self.pos_conv = nn.Conv1d(\n",
        "            self.embedding_dim,\n",
        "            self.embedding_dim,\n",
        "            kernel_size=args.conv_pos,\n",
        "            padding=args.conv_pos // 2,\n",
        "            groups=args.conv_pos_groups,\n",
        "        )\n",
        "        dropout = 0\n",
        "        std = math.sqrt((4 * (1.0 - dropout)) / (args.conv_pos * self.embedding_dim))\n",
        "        nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\n",
        "        nn.init.constant_(self.pos_conv.bias, 0)\n",
        "\n",
        "        self.pos_conv = nn.utils.weight_norm(self.pos_conv, name=\"weight\", dim=2)\n",
        "        self.pos_conv = nn.Sequential(self.pos_conv, SamePad(args.conv_pos), nn.GELU())\n",
        "\n",
        "        if hasattr(args, \"relative_position_embedding\"):\n",
        "            self.relative_position_embedding = args.relative_position_embedding\n",
        "            self.num_buckets = args.num_buckets\n",
        "            self.max_distance = args.max_distance\n",
        "        else:\n",
        "            self.relative_position_embedding = False\n",
        "            self.num_buckets = 0\n",
        "            self.max_distance = 0\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerSentenceEncoderLayer(\n",
        "                    embedding_dim=self.embedding_dim,\n",
        "                    ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
        "                    num_attention_heads=args.encoder_attention_heads,\n",
        "                    dropout=self.dropout,\n",
        "                    attention_dropout=args.attention_dropout,\n",
        "                    activation_dropout=args.activation_dropout,\n",
        "                    activation_fn=args.activation_fn,\n",
        "                    layer_norm_first=args.layer_norm_first,\n",
        "                    deep_norm=args.deep_norm,\n",
        "                    has_relative_attention_bias=self.relative_position_embedding,\n",
        "                    num_buckets=self.num_buckets,\n",
        "                    max_distance=self.max_distance,\n",
        "                    gru_rel_pos=args.gru_rel_pos,\n",
        "                    encoder_layers=args.encoder_layers,\n",
        "                )\n",
        "                for i in range(args.encoder_layers)\n",
        "            ]\n",
        "        )\n",
        "        if self.relative_position_embedding:\n",
        "            for i in range(1, args.encoder_layers):\n",
        "                del self.layers[i].self_attn.relative_attention_bias\n",
        "                self.layers[i].self_attn.relative_attention_bias = self.layers[0].self_attn.relative_attention_bias\n",
        "\n",
        "        self.layer_norm_first = args.layer_norm_first\n",
        "        self.layer_norm = LayerNorm(self.embedding_dim)\n",
        "        self.layerdrop = args.encoder_layerdrop\n",
        "\n",
        "        self.apply(init_bert_params)\n",
        "\n",
        "        if args.deep_norm:\n",
        "            deep_norm_beta = math.pow(8 * args.encoder_layers, -1 / 4)\n",
        "            for i in range(args.encoder_layers):\n",
        "                nn.init.xavier_normal_(self.layers[i].self_attn.k_proj.weight, gain=1)\n",
        "                nn.init.xavier_normal_(self.layers[i].self_attn.v_proj.weight, gain=deep_norm_beta)\n",
        "                nn.init.xavier_normal_(self.layers[i].self_attn.q_proj.weight, gain=1)\n",
        "                nn.init.xavier_normal_(self.layers[i].self_attn.out_proj.weight, gain=deep_norm_beta)\n",
        "                nn.init.xavier_normal_(self.layers[i].fc1.weight, gain=deep_norm_beta)\n",
        "                nn.init.xavier_normal_(self.layers[i].fc2.weight, gain=deep_norm_beta)\n",
        "\n",
        "        self.layer_wise_gradient_decay_ratio = getattr(args, \"layer_wise_gradient_decay_ratio\", 1)\n",
        "\n",
        "    def forward(self, x, padding_mask=None, layer=None):\n",
        "        x, layer_results = self.extract_features(x, padding_mask, layer)\n",
        "\n",
        "        if self.layer_norm_first and layer is None:\n",
        "            x = self.layer_norm(x)\n",
        "\n",
        "        return x, layer_results\n",
        "\n",
        "    def extract_features(self, x, padding_mask=None, tgt_layer=None):\n",
        "\n",
        "        if padding_mask is not None:\n",
        "            x[padding_mask] = 0\n",
        "\n",
        "        x_conv = self.pos_conv(x.transpose(1, 2))\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "        x = x + x_conv\n",
        "\n",
        "        if not self.layer_norm_first:\n",
        "            x = self.layer_norm(x)\n",
        "\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        layer_results = []\n",
        "        z = None\n",
        "        if tgt_layer is not None:\n",
        "            layer_results.append((x, z))\n",
        "        r = None\n",
        "        pos_bias = None\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if self.layer_wise_gradient_decay_ratio != 1.0:\n",
        "                x = GradMultiply.apply(x, self.layer_wise_gradient_decay_ratio)\n",
        "            dropout_probability = np.random.random()\n",
        "            if not self.training or (dropout_probability > self.layerdrop):\n",
        "                x, z, pos_bias = layer(x, self_attn_padding_mask=padding_mask, need_weights=False, pos_bias=pos_bias)\n",
        "            if tgt_layer is not None:\n",
        "                layer_results.append((x, z))\n",
        "            if i == tgt_layer:\n",
        "                r = x\n",
        "                break\n",
        "\n",
        "        if r is not None:\n",
        "            x = r\n",
        "\n",
        "        # T x B x C -> B x T x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        return x, layer_results\n",
        "\n",
        "\n",
        "class TransformerSentenceEncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dim: float = 768,\n",
        "            ffn_embedding_dim: float = 3072,\n",
        "            num_attention_heads: float = 8,\n",
        "            dropout: float = 0.1,\n",
        "            attention_dropout: float = 0.1,\n",
        "            activation_dropout: float = 0.1,\n",
        "            activation_fn: str = \"relu\",\n",
        "            layer_norm_first: bool = False,\n",
        "            deep_norm: bool = False,\n",
        "            has_relative_attention_bias: bool = False,\n",
        "            num_buckets: int = 0,\n",
        "            max_distance: int = 0,\n",
        "            rescale_init: bool = False,\n",
        "            gru_rel_pos: bool = False,\n",
        "            encoder_layers: int = 0,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.dropout = dropout\n",
        "        self.activation_dropout = activation_dropout\n",
        "\n",
        "        self.activation_name = activation_fn\n",
        "        self.activation_fn = get_activation_fn(activation_fn)\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            self.embedding_dim,\n",
        "            num_attention_heads,\n",
        "            dropout=attention_dropout,\n",
        "            self_attention=True,\n",
        "            has_relative_attention_bias=has_relative_attention_bias,\n",
        "            num_buckets=num_buckets,\n",
        "            max_distance=max_distance,\n",
        "            rescale_init=rescale_init,\n",
        "            gru_rel_pos=gru_rel_pos,\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(self.activation_dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer_norm_first = layer_norm_first\n",
        "\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n",
        "\n",
        "        if self.activation_name == \"glu\":\n",
        "            self.fc1 = GLU_Linear(self.embedding_dim, ffn_embedding_dim, \"swish\")\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
        "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
        "\n",
        "        self.final_layer_norm = LayerNorm(self.embedding_dim)\n",
        "\n",
        "        self.deep_norm = deep_norm\n",
        "        if self.deep_norm:\n",
        "            self.deep_norm_alpha = math.pow(2 * encoder_layers, 1 / 4)\n",
        "        else:\n",
        "            self.deep_norm_alpha = 1\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor,\n",
        "            self_attn_mask: torch.Tensor = None,\n",
        "            self_attn_padding_mask: torch.Tensor = None,\n",
        "            need_weights: bool = False,\n",
        "            pos_bias=None\n",
        "    ):\n",
        "        residual = x\n",
        "\n",
        "        if self.layer_norm_first:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "            x, attn, pos_bias = self.self_attn(\n",
        "                query=x,\n",
        "                key=x,\n",
        "                value=x,\n",
        "                key_padding_mask=self_attn_padding_mask,\n",
        "                need_weights=False,\n",
        "                attn_mask=self_attn_mask,\n",
        "                position_bias=pos_bias\n",
        "            )\n",
        "            x = self.dropout1(x)\n",
        "            x = residual + x\n",
        "\n",
        "            residual = x\n",
        "            x = self.final_layer_norm(x)\n",
        "            if self.activation_name == \"glu\":\n",
        "                x = self.fc1(x)\n",
        "            else:\n",
        "                x = self.activation_fn(self.fc1(x))\n",
        "            x = self.dropout2(x)\n",
        "            x = self.fc2(x)\n",
        "            x = self.dropout3(x)\n",
        "            x = residual + x\n",
        "        else:\n",
        "            x, attn, pos_bias = self.self_attn(\n",
        "                query=x,\n",
        "                key=x,\n",
        "                value=x,\n",
        "                key_padding_mask=self_attn_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                attn_mask=self_attn_mask,\n",
        "                position_bias=pos_bias\n",
        "            )\n",
        "\n",
        "            x = self.dropout1(x)\n",
        "            x = residual * self.deep_norm_alpha + x\n",
        "\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "            residual = x\n",
        "            if self.activation_name == \"glu\":\n",
        "                x = self.fc1(x)\n",
        "            else:\n",
        "                x = self.activation_fn(self.fc1(x))\n",
        "            x = self.dropout2(x)\n",
        "            x = self.fc2(x)\n",
        "            x = self.dropout3(x)\n",
        "            x = residual * self.deep_norm_alpha + x\n",
        "            x = self.final_layer_norm(x)\n",
        "\n",
        "        return x, attn, pos_bias\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention.\n",
        "\n",
        "    See \"Attention Is All You Need\" for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            kdim=None,\n",
        "            vdim=None,\n",
        "            dropout=0.0,\n",
        "            bias=True,\n",
        "            add_bias_kv=False,\n",
        "            add_zero_attn=False,\n",
        "            self_attention=False,\n",
        "            encoder_decoder_attention=False,\n",
        "            q_noise=0.0,\n",
        "            qn_block_size=8,\n",
        "            has_relative_attention_bias=False,\n",
        "            num_buckets=32,\n",
        "            max_distance=128,\n",
        "            gru_rel_pos=False,\n",
        "            rescale_init=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_module = nn.Dropout(dropout)\n",
        "\n",
        "        self.has_relative_attention_bias = has_relative_attention_bias\n",
        "        self.num_buckets = num_buckets\n",
        "        self.max_distance = max_distance\n",
        "        if self.has_relative_attention_bias:\n",
        "            self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n",
        "\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.q_head_dim = self.head_dim\n",
        "        self.k_head_dim = self.head_dim\n",
        "        assert (\n",
        "                self.head_dim * num_heads == self.embed_dim\n",
        "        ), \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.self_attention = self_attention\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "\n",
        "        assert not self.self_attention or self.qkv_same_dim, (\n",
        "            \"Self-attention requires query, key and \" \"value to be of the same size\"\n",
        "        )\n",
        "\n",
        "        k_bias = True\n",
        "        if rescale_init:\n",
        "            k_bias = False\n",
        "\n",
        "        k_embed_dim = embed_dim\n",
        "        q_embed_dim = embed_dim\n",
        "\n",
        "        self.k_proj = quant_noise(\n",
        "            nn.Linear(self.kdim, k_embed_dim, bias=k_bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.v_proj = quant_noise(\n",
        "            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.q_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, q_embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        self.out_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self.gru_rel_pos = gru_rel_pos\n",
        "        if self.gru_rel_pos:\n",
        "            self.grep_linear = nn.Linear(self.q_head_dim, 8)\n",
        "            self.grep_a = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.qkv_same_dim:\n",
        "            # Empirically observed the convergence to be much better with\n",
        "            # the scaled initialization\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        if self.out_proj.bias is not None:\n",
        "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
        "        if self.bias_k is not None:\n",
        "            nn.init.xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            nn.init.xavier_normal_(self.bias_v)\n",
        "        if self.has_relative_attention_bias:\n",
        "            nn.init.xavier_normal_(self.relative_attention_bias.weight)\n",
        "\n",
        "    def _relative_positions_bucket(self, relative_positions, bidirectional=True):\n",
        "        num_buckets = self.num_buckets\n",
        "        max_distance = self.max_distance\n",
        "        relative_buckets = 0\n",
        "\n",
        "        if bidirectional:\n",
        "            num_buckets = num_buckets // 2\n",
        "            relative_buckets += (relative_positions > 0).to(torch.long) * num_buckets\n",
        "            relative_positions = torch.abs(relative_positions)\n",
        "        else:\n",
        "            relative_positions = -torch.min(relative_positions, torch.zeros_like(relative_positions))\n",
        "\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = relative_positions < max_exact\n",
        "\n",
        "        relative_postion_if_large = max_exact + (\n",
        "                torch.log(relative_positions.float() / max_exact)\n",
        "                / math.log(max_distance / max_exact)\n",
        "                * (num_buckets - max_exact)\n",
        "        ).to(torch.long)\n",
        "        relative_postion_if_large = torch.min(\n",
        "            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n",
        "        )\n",
        "\n",
        "        relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large)\n",
        "        return relative_buckets\n",
        "\n",
        "    def compute_bias(self, query_length, key_length):\n",
        "        context_position = torch.arange(query_length, dtype=torch.long)[:, None]\n",
        "        memory_position = torch.arange(key_length, dtype=torch.long)[None, :]\n",
        "        relative_position = memory_position - context_position\n",
        "        relative_position_bucket = self._relative_positions_bucket(\n",
        "            relative_position,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        relative_position_bucket = relative_position_bucket.to(self.relative_attention_bias.weight.device)\n",
        "        values = self.relative_attention_bias(relative_position_bucket)\n",
        "        values = values.permute([2, 0, 1])\n",
        "        return values\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query,\n",
        "            key: Optional[Tensor],\n",
        "            value: Optional[Tensor],\n",
        "            key_padding_mask: Optional[Tensor] = None,\n",
        "            incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
        "            need_weights: bool = True,\n",
        "            static_kv: bool = False,\n",
        "            attn_mask: Optional[Tensor] = None,\n",
        "            before_softmax: bool = False,\n",
        "            need_head_weights: bool = False,\n",
        "            position_bias: Optional[Tensor] = None\n",
        "    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n",
        "        \"\"\"Input shape: Time x Batch x Channel\n",
        "\n",
        "        Args:\n",
        "            key_padding_mask (ByteTensor, optional): mask to exclude\n",
        "                keys that are pads, of shape `(batch, src_len)`, where\n",
        "                padding elements are indicated by 1s.\n",
        "            need_weights (bool, optional): return the attention weights,\n",
        "                averaged over heads (default: False).\n",
        "            attn_mask (ByteTensor, optional): typically used to\n",
        "                implement causal attention, where the mask prevents the\n",
        "                attention from looking forward in time (default: None).\n",
        "            before_softmax (bool, optional): return the raw attention\n",
        "                weights and values before the attention softmax.\n",
        "            need_head_weights (bool, optional): return the attention\n",
        "                weights for each head. Implies *need_weights*. Default:\n",
        "                return the average attention weights over all heads.\n",
        "        \"\"\"\n",
        "        if need_head_weights:\n",
        "            need_weights = True\n",
        "\n",
        "        is_tpu = query.device.type == \"xla\"\n",
        "\n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        src_len = tgt_len\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        if key is not None:\n",
        "            src_len, key_bsz, _ = key.size()\n",
        "            if not torch.jit.is_scripting():\n",
        "                assert key_bsz == bsz\n",
        "                assert value is not None\n",
        "                assert src_len, bsz == value.shape[:2]\n",
        "\n",
        "        if self.has_relative_attention_bias and position_bias is None:\n",
        "            position_bias = self.compute_bias(tgt_len, src_len)\n",
        "            position_bias = position_bias.unsqueeze(0).repeat(bsz, 1, 1, 1).view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if incremental_state is not None:\n",
        "            saved_state = self._get_input_buffer(incremental_state)\n",
        "            if saved_state is not None and \"prev_key\" in saved_state:\n",
        "                # previous time steps are cached - no need to recompute\n",
        "                # key and value if they are static\n",
        "                if static_kv:\n",
        "                    assert self.encoder_decoder_attention and not self.self_attention\n",
        "                    key = value = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "\n",
        "        if self.self_attention:\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "        elif self.encoder_decoder_attention:\n",
        "            # encoder-decoder attention\n",
        "            q = self.q_proj(query)\n",
        "            if key is None:\n",
        "                assert value is None\n",
        "                k = v = None\n",
        "            else:\n",
        "                k = self.k_proj(key)\n",
        "                v = self.v_proj(key)\n",
        "\n",
        "        else:\n",
        "            assert key is not None and value is not None\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(key)\n",
        "            v = self.v_proj(value)\n",
        "        q *= self.scaling\n",
        "        alpha = 32\n",
        "        q *= 1 / alpha\n",
        "\n",
        "        if self.bias_k is not None:\n",
        "            assert self.bias_v is not None\n",
        "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
        "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        q = (\n",
        "            q.contiguous()\n",
        "                .view(tgt_len, bsz * self.num_heads, self.q_head_dim)\n",
        "                .transpose(0, 1)\n",
        "        )\n",
        "        if k is not None:\n",
        "            k = (\n",
        "                k.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.k_head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "        if v is not None:\n",
        "            v = (\n",
        "                v.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "\n",
        "        if saved_state is not None:\n",
        "            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "            if \"prev_key\" in saved_state:\n",
        "                _prev_key = saved_state[\"prev_key\"]\n",
        "                assert _prev_key is not None\n",
        "                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    k = prev_key\n",
        "                else:\n",
        "                    assert k is not None\n",
        "                    k = torch.cat([prev_key, k], dim=1)\n",
        "                src_len = k.size(1)\n",
        "            if \"prev_value\" in saved_state:\n",
        "                _prev_value = saved_state[\"prev_value\"]\n",
        "                assert _prev_value is not None\n",
        "                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    v = prev_value\n",
        "                else:\n",
        "                    assert v is not None\n",
        "                    v = torch.cat([prev_value, v], dim=1)\n",
        "            prev_key_padding_mask: Optional[Tensor] = None\n",
        "            if \"prev_key_padding_mask\" in saved_state:\n",
        "                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n",
        "            assert k is not None and v is not None\n",
        "            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                prev_key_padding_mask=prev_key_padding_mask,\n",
        "                batch_size=bsz,\n",
        "                src_len=k.size(1),\n",
        "                static_kv=static_kv,\n",
        "            )\n",
        "\n",
        "            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n",
        "            # In this branch incremental_state is never None\n",
        "            assert incremental_state is not None\n",
        "            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n",
        "        assert k is not None\n",
        "        assert k.size(1) == src_len\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism\n",
        "        # not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n",
        "            key_padding_mask = None\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            assert key_padding_mask.size(0) == bsz\n",
        "            assert key_padding_mask.size(1) == src_len\n",
        "\n",
        "        if self.add_zero_attn:\n",
        "            assert v is not None\n",
        "            src_len += 1\n",
        "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
        "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n",
        "                            key_padding_mask\n",
        "                        ),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        attn_weights = (attn_weights - attn_weights.max(dim=-1, keepdim=True)[0]) * alpha\n",
        "        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n",
        "\n",
        "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "            attn_weights += attn_mask\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            if not is_tpu:\n",
        "                attn_weights = attn_weights.masked_fill(\n",
        "                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n",
        "                    float(\"-inf\"),\n",
        "                )\n",
        "            else:\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if before_softmax:\n",
        "            return attn_weights, v, position_bias\n",
        "\n",
        "        if position_bias is not None:\n",
        "            attn_mask_rel_pos = position_bias\n",
        "            if self.gru_rel_pos == 1:\n",
        "                query_layer = q.view(bsz, self.num_heads, tgt_len, self.q_head_dim) * alpha / self.scaling\n",
        "                _B, _H, _L, __ = query_layer.size()\n",
        "                gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                    _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                attn_mask_rel_pos = gate_a_1.view(bsz * self.num_heads, tgt_len, 1) * position_bias\n",
        "\n",
        "            attn_mask_rel_pos = attn_mask_rel_pos.view(attn_weights.size())\n",
        "\n",
        "            attn_weights = attn_weights + attn_mask_rel_pos\n",
        "\n",
        "        attn_weights_float = F.softmax(\n",
        "            attn_weights, dim=-1\n",
        "        )\n",
        "        attn_weights = attn_weights_float.type_as(attn_weights)\n",
        "        attn_probs = self.dropout_module(attn_weights)\n",
        "\n",
        "        assert v is not None\n",
        "        attn = torch.bmm(attn_probs, v)\n",
        "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
        "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn = self.out_proj(attn)\n",
        "        attn_weights: Optional[Tensor] = None\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights_float.view(\n",
        "                bsz, self.num_heads, tgt_len, src_len\n",
        "            ).transpose(1, 0)\n",
        "            if not need_head_weights:\n",
        "                # average attention weights over heads\n",
        "                attn_weights = attn_weights.mean(dim=0)\n",
        "\n",
        "        return attn, attn_weights, position_bias\n",
        "\n",
        "    @staticmethod\n",
        "    def _append_prev_key_padding_mask(\n",
        "            key_padding_mask: Optional[Tensor],\n",
        "            prev_key_padding_mask: Optional[Tensor],\n",
        "            batch_size: int,\n",
        "            src_len: int,\n",
        "            static_kv: bool,\n",
        "    ) -> Optional[Tensor]:\n",
        "        # saved key padding masks have shape (bsz, seq_len)\n",
        "        if prev_key_padding_mask is not None and static_kv:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n",
        "            new_key_padding_mask = torch.cat(\n",
        "                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n",
        "            )\n",
        "        # During incremental decoding, as the padding token enters and\n",
        "        # leaves the frame, there will be a time when prev or current\n",
        "        # is None\n",
        "        elif prev_key_padding_mask is not None:\n",
        "            if src_len > prev_key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n",
        "                    device=prev_key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [prev_key_padding_mask.float(), filler.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = prev_key_padding_mask.float()\n",
        "        elif key_padding_mask is not None:\n",
        "            if src_len > key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - key_padding_mask.size(1)),\n",
        "                    device=key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [filler.float(), key_padding_mask.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = key_padding_mask.float()\n",
        "        else:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        return new_key_padding_mask\n",
        "\n",
        "    def _get_input_buffer(\n",
        "            self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n",
        "    ) -> Dict[str, Optional[Tensor]]:\n",
        "        result = self.get_incremental_state(incremental_state, \"attn_state\")\n",
        "        if result is not None:\n",
        "            return result\n",
        "        else:\n",
        "            empty_result: Dict[str, Optional[Tensor]] = {}\n",
        "            return empty_result\n",
        "\n",
        "    def _set_input_buffer(\n",
        "            self,\n",
        "            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n",
        "            buffer: Dict[str, Optional[Tensor]],\n",
        "    ):\n",
        "        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n",
        "\n",
        "    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n",
        "        return attn_weights\n",
        "\n",
        "\n",
        "def init_bert_params(module):\n",
        "    \"\"\"\n",
        "    Initialize the weights specific to the BERT Model.\n",
        "    This overrides the default initializations depending on the specified arguments.\n",
        "        1. If normal_init_linear_weights is set then weights of linear\n",
        "           layer will be initialized using the normal distribution and\n",
        "           bais will be set to the specified value.\n",
        "        2. If normal_init_embed_weights is set then weights of embedding\n",
        "           layer will be initialized using the normal distribution.\n",
        "        3. If normal_init_proj_weights is set then weights of\n",
        "           in_project_weight for MultiHeadAttention initialized using\n",
        "           the normal distribution (to be validated).\n",
        "    \"\"\"\n",
        "\n",
        "    def normal_(data):\n",
        "        # with FSDP, module params will be on CUDA, so we cast them back to CPU\n",
        "        # so that the RNG is consistent with and without FSDP\n",
        "        data.copy_(\n",
        "            data.cpu().normal_(mean=0.0, std=0.02).to(data.device)\n",
        "        )\n",
        "\n",
        "    if isinstance(module, nn.Linear):\n",
        "        normal_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    if isinstance(module, nn.Embedding):\n",
        "        normal_(module.weight.data)\n",
        "        if module.padding_idx is not None:\n",
        "            module.weight.data[module.padding_idx].zero_()\n",
        "    if isinstance(module, MultiheadAttention):\n",
        "        normal_(module.q_proj.weight.data)\n",
        "        normal_(module.k_proj.weight.data)\n",
        "        normal_(module.v_proj.weight.data)"
      ],
      "metadata": {
        "id": "27uFkpRD1zpS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "quantizer.py"
      ],
      "metadata": {
        "id": "vAV12tLm2UWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# BEATs: Audio Pre-Training with Acoustic Tokenizers (https://arxiv.org/abs/2212.09058)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/beats\n",
        "# Copyright (c) 2022 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on VQGAN code bases\n",
        "# https://github.com/CompVis/taming-transformers\n",
        "# --------------------------------------------------------'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as distributed\n",
        "\n",
        "try:\n",
        "    from einops import rearrange, repeat\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def l2norm(t):\n",
        "    return F.normalize(t, p=2, dim=-1)\n",
        "\n",
        "\n",
        "def ema_inplace(moving_avg, new, decay):\n",
        "    moving_avg.data.mul_(decay).add_(new, alpha=(1 - decay))\n",
        "\n",
        "\n",
        "def sample_vectors(samples, num):\n",
        "    num_samples, device = samples.shape[0], samples.device\n",
        "\n",
        "    if num_samples >= num:\n",
        "        indices = torch.randperm(num_samples, device=device)[:num]\n",
        "    else:\n",
        "        indices = torch.randint(0, num_samples, (num,), device=device)\n",
        "\n",
        "    return samples[indices]\n",
        "\n",
        "\n",
        "def kmeans(samples, num_clusters, num_iters=10, use_cosine_sim=False):\n",
        "    dim, dtype, device = samples.shape[-1], samples.dtype, samples.device\n",
        "\n",
        "    means = sample_vectors(samples, num_clusters)\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        if use_cosine_sim:\n",
        "            dists = samples @ means.t()\n",
        "        else:\n",
        "            diffs = rearrange(samples, 'n d -> n () d') \\\n",
        "                    - rearrange(means, 'c d -> () c d')\n",
        "            dists = -(diffs ** 2).sum(dim=-1)\n",
        "\n",
        "        buckets = dists.max(dim=-1).indices\n",
        "        bins = torch.bincount(buckets, minlength=num_clusters)\n",
        "        zero_mask = bins == 0\n",
        "        bins_min_clamped = bins.masked_fill(zero_mask, 1)\n",
        "\n",
        "        new_means = buckets.new_zeros(num_clusters, dim, dtype=dtype)\n",
        "        new_means.scatter_add_(0, repeat(buckets, 'n -> n d', d=dim), samples)\n",
        "        new_means = new_means / bins_min_clamped[..., None]\n",
        "\n",
        "        if use_cosine_sim:\n",
        "            new_means = l2norm(new_means)\n",
        "\n",
        "        means = torch.where(zero_mask[..., None], means, new_means)\n",
        "\n",
        "    return means, bins\n",
        "\n",
        "\n",
        "class EmbeddingEMA(nn.Module):\n",
        "    def __init__(self, num_tokens, codebook_dim, decay=0.99, eps=1e-5, kmeans_init=True, codebook_init_path=''):\n",
        "        super().__init__()\n",
        "        self.num_tokens = num_tokens\n",
        "        self.codebook_dim = codebook_dim\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        if codebook_init_path == '':\n",
        "            if not kmeans_init:\n",
        "                weight = torch.randn(num_tokens, codebook_dim)\n",
        "                weight = l2norm(weight)\n",
        "            else:\n",
        "                weight = torch.zeros(num_tokens, codebook_dim)\n",
        "            self.register_buffer('initted', torch.Tensor([not kmeans_init]))\n",
        "        else:\n",
        "            print(f\"load init codebook weight from {codebook_init_path}\")\n",
        "            codebook_ckpt_weight = torch.load(codebook_init_path, map_location='cpu')\n",
        "            weight = codebook_ckpt_weight.clone()\n",
        "            self.register_buffer('initted', torch.Tensor([True]))\n",
        "\n",
        "        self.weight = nn.Parameter(weight, requires_grad=False)\n",
        "        self.cluster_size = nn.Parameter(torch.zeros(num_tokens), requires_grad=False)\n",
        "        self.embed_avg = nn.Parameter(weight.clone(), requires_grad=False)\n",
        "        # self.register_buffer('initted', torch.Tensor([not kmeans_init]))\n",
        "        self.update = True\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def init_embed_(self, data):\n",
        "        if self.initted:\n",
        "            return\n",
        "        print(\"Performing Kemans init for codebook\")\n",
        "        embed, cluster_size = kmeans(data, self.num_tokens, 10, use_cosine_sim=True)\n",
        "        self.weight.data.copy_(embed)\n",
        "        self.cluster_size.data.copy_(cluster_size)\n",
        "        self.initted.data.copy_(torch.Tensor([True]))\n",
        "\n",
        "    def forward(self, embed_id):\n",
        "        return F.embedding(embed_id, self.weight)\n",
        "\n",
        "    def cluster_size_ema_update(self, new_cluster_size):\n",
        "        self.cluster_size.data.mul_(self.decay).add_(new_cluster_size, alpha=1 - self.decay)\n",
        "\n",
        "    def embed_avg_ema_update(self, new_embed_avg):\n",
        "        self.embed_avg.data.mul_(self.decay).add_(new_embed_avg, alpha=1 - self.decay)\n",
        "\n",
        "    def weight_update(self, num_tokens):\n",
        "        n = self.cluster_size.sum()\n",
        "        smoothed_cluster_size = (\n",
        "                (self.cluster_size + self.eps) / (n + num_tokens * self.eps) * n\n",
        "        )\n",
        "        # normalize embedding average with smoothed cluster size\n",
        "        embed_normalized = self.embed_avg / smoothed_cluster_size.unsqueeze(1)\n",
        "        # embed_normalized = l2norm(self.embed_avg / smoothed_cluster_size.unsqueeze(1))\n",
        "        self.weight.data.copy_(embed_normalized)\n",
        "\n",
        "\n",
        "def norm_ema_inplace(moving_avg, new, decay):\n",
        "    moving_avg.data.mul_(decay).add_(new, alpha=(1 - decay))\n",
        "    moving_avg.data.copy_(l2norm(moving_avg.data))\n",
        "\n",
        "\n",
        "class NormEMAVectorQuantizer(nn.Module):\n",
        "    def __init__(self, n_embed, embedding_dim, beta, decay=0.99, eps=1e-5,\n",
        "                 statistic_code_usage=True, kmeans_init=False, codebook_init_path=''):\n",
        "        super().__init__()\n",
        "        self.codebook_dim = embedding_dim\n",
        "        self.num_tokens = n_embed\n",
        "        self.beta = beta\n",
        "        self.decay = decay\n",
        "\n",
        "        # learnable = True if orthogonal_reg_weight > 0 else False\n",
        "        self.embedding = EmbeddingEMA(self.num_tokens, self.codebook_dim, decay, eps, kmeans_init, codebook_init_path)\n",
        "\n",
        "        self.statistic_code_usage = statistic_code_usage\n",
        "        if statistic_code_usage:\n",
        "            self.register_buffer('cluster_size', torch.zeros(n_embed))\n",
        "        if distributed.is_available() and distributed.is_initialized():\n",
        "            print(\"ddp is enable, so use ddp_reduce to sync the statistic_code_usage for each gpu!\")\n",
        "            self.all_reduce_fn = distributed.all_reduce\n",
        "        else:\n",
        "            self.all_reduce_fn = nn.Identity()\n",
        "\n",
        "    def reset_cluster_size(self, device):\n",
        "        if self.statistic_code_usage:\n",
        "            self.register_buffer('cluster_size', torch.zeros(self.num_tokens))\n",
        "            self.cluster_size = self.cluster_size.to(device)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        # z, 'b c h w -> b h w c'\n",
        "        # z = rearrange(z, 'b c h w -> b h w c')\n",
        "        # z = z.transpose(1, 2)\n",
        "        z = l2norm(z)\n",
        "        z_flattened = z.reshape(-1, self.codebook_dim)\n",
        "\n",
        "        self.embedding.init_embed_(z_flattened)\n",
        "\n",
        "        d = z_flattened.pow(2).sum(dim=1, keepdim=True) + \\\n",
        "            self.embedding.weight.pow(2).sum(dim=1) - 2 * \\\n",
        "            torch.einsum('bd,nd->bn', z_flattened, self.embedding.weight)  # 'n d -> d n'\n",
        "\n",
        "        encoding_indices = torch.argmin(d, dim=1)\n",
        "\n",
        "        z_q = self.embedding(encoding_indices).view(z.shape)\n",
        "\n",
        "        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)\n",
        "\n",
        "        if not self.training:\n",
        "            with torch.no_grad():\n",
        "                cluster_size = encodings.sum(0)\n",
        "                self.all_reduce_fn(cluster_size)\n",
        "                ema_inplace(self.cluster_size, cluster_size, self.decay)\n",
        "\n",
        "        if self.training and self.embedding.update:\n",
        "            # EMA cluster size\n",
        "\n",
        "            bins = encodings.sum(0)\n",
        "            self.all_reduce_fn(bins)\n",
        "\n",
        "            # self.embedding.cluster_size_ema_update(bins)\n",
        "            ema_inplace(self.cluster_size, bins, self.decay)\n",
        "\n",
        "            zero_mask = (bins == 0)\n",
        "            bins = bins.masked_fill(zero_mask, 1.)\n",
        "\n",
        "            embed_sum = z_flattened.t() @ encodings\n",
        "            self.all_reduce_fn(embed_sum)\n",
        "\n",
        "            embed_normalized = (embed_sum / bins.unsqueeze(0)).t()\n",
        "            embed_normalized = l2norm(embed_normalized)\n",
        "\n",
        "            embed_normalized = torch.where(zero_mask[..., None], self.embedding.weight,\n",
        "                                           embed_normalized)\n",
        "            norm_ema_inplace(self.embedding.weight, embed_normalized, self.decay)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * F.mse_loss(z_q.detach(), z)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        # z_q, 'b h w c -> b c h w'\n",
        "        # z_q = rearrange(z_q, 'b h w c -> b c h w')\n",
        "        # z_q = z_q.transpose(1, 2)\n",
        "        return z_q, loss, encoding_indices"
      ],
      "metadata": {
        "id": "6vgkeKWO2dWk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers.py"
      ],
      "metadata": {
        "id": "AyIsDGy12jc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# BEATs: Audio Pre-Training with Acoustic Tokenizers (https://arxiv.org/abs/2212.09058)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/beats\n",
        "# Copyright (c) 2022 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import LayerNorm\n",
        "import torchaudio.compliance.kaldi as ta_kaldi\n",
        "\n",
        "import logging\n",
        "from typing import Optional\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class TokenizersConfig:\n",
        "    def __init__(self, cfg=None):\n",
        "        self.input_patch_size: int = 256  # path size of patch embedding, 16x16 in paper implementation\n",
        "        self.embed_dim: int = 512  # patch embedding dimension\n",
        "        self.conv_bias: bool = False  # include bias in conv encoder\n",
        "\n",
        "        self.encoder_layers: int = 12  # num encoder layers in the transformer\n",
        "        self.encoder_embed_dim: int = 768  # encoder embedding dimension\n",
        "        self.encoder_ffn_embed_dim: int = 3072  # encoder embedding dimension for FFN\n",
        "        self.encoder_attention_heads: int = 12  # num encoder attention heads\n",
        "        self.activation_fn: str = \"gelu\"  # activation function to use\n",
        "\n",
        "        self.layer_norm_first: bool = False  # apply layernorm first in the transformer\n",
        "        self.deep_norm: bool = False  # apply deep_norm first in the transformer\n",
        "\n",
        "        # dropouts\n",
        "        self.dropout: float = 0.1  # dropout probability for the transformer\n",
        "        self.attention_dropout: float = 0.1  # dropout probability for attention weights\n",
        "        self.activation_dropout: float = 0.0  # dropout probability after activation in FFN\n",
        "        self.encoder_layerdrop: float = 0.0  # probability of dropping a tarnsformer layer\n",
        "        self.dropout_input: float = 0.0  # dropout to apply to the input (after feat extr)\n",
        "\n",
        "        # positional embeddings\n",
        "        self.conv_pos: int = 128  # number of filters for convolutional positional embeddings\n",
        "        self.conv_pos_groups: int = 16  # number of groups for convolutional positional embedding\n",
        "\n",
        "        # relative position embedding\n",
        "        self.relative_position_embedding: bool = False  # apply relative position embedding\n",
        "        self.num_buckets: int = 320  # number of buckets for relative position embedding\n",
        "        self.max_distance: int = 1280  # maximum distance for relative position embedding\n",
        "        self.gru_rel_pos: bool = False  # apply gated relative position embedding\n",
        "\n",
        "        # quantizer\n",
        "        self.quant_n: int = 1024 # codebook number in quantizer\n",
        "        self.quant_dim: int = 256    # codebook dimension in quantizer\n",
        "\n",
        "        if cfg is not None:\n",
        "            self.update(cfg)\n",
        "\n",
        "    def update(self, cfg: dict):\n",
        "        self.__dict__.update(cfg)\n",
        "\n",
        "\n",
        "class Tokenizers(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            cfg: TokenizersConfig,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        logger.info(f\"Tokenizers Config: {cfg.__dict__}\")\n",
        "\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.embed = cfg.embed_dim\n",
        "        self.post_extract_proj = (\n",
        "            nn.Linear(self.embed, cfg.encoder_embed_dim)\n",
        "            if self.embed != cfg.encoder_embed_dim\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.input_patch_size = cfg.input_patch_size\n",
        "        self.patch_embedding = nn.Conv2d(1, self.embed, kernel_size=self.input_patch_size, stride=self.input_patch_size,\n",
        "                                         bias=cfg.conv_bias)\n",
        "\n",
        "        self.dropout_input = nn.Dropout(cfg.dropout_input)\n",
        "\n",
        "        assert not cfg.deep_norm or not cfg.layer_norm_first\n",
        "        self.encoder = TransformerEncoder(cfg)\n",
        "        self.layer_norm = LayerNorm(self.embed)\n",
        "\n",
        "        self.quantize = NormEMAVectorQuantizer(\n",
        "            n_embed=cfg.quant_n, embedding_dim=cfg.quant_dim, beta=1.0, kmeans_init=True, decay=0.99,\n",
        "        )\n",
        "        self.quant_n = cfg.quant_n\n",
        "        self.quantize_layer = nn.Sequential(\n",
        "            nn.Linear(cfg.encoder_embed_dim, cfg.encoder_embed_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(cfg.encoder_embed_dim, cfg.quant_dim)  # for quantize\n",
        "        )\n",
        "\n",
        "    def forward_padding_mask(\n",
        "            self,\n",
        "            features: torch.Tensor,\n",
        "            padding_mask: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        extra = padding_mask.size(1) % features.size(1)\n",
        "        if extra > 0:\n",
        "            padding_mask = padding_mask[:, :-extra]\n",
        "        padding_mask = padding_mask.view(\n",
        "            padding_mask.size(0), features.size(1), -1\n",
        "        )\n",
        "        padding_mask = padding_mask.all(-1)\n",
        "        return padding_mask\n",
        "\n",
        "    def preprocess(\n",
        "            self,\n",
        "            source: torch.Tensor,\n",
        "            fbank_mean: float = 15.41663,\n",
        "            fbank_std: float = 6.55582,\n",
        "    ) -> torch.Tensor:\n",
        "        fbanks = []\n",
        "        for waveform in source:\n",
        "            waveform = waveform.unsqueeze(0) * 2 ** 15\n",
        "            fbank = ta_kaldi.fbank(waveform, num_mel_bins=128, sample_frequency=16000, frame_length=25, frame_shift=10)\n",
        "            fbanks.append(fbank)\n",
        "        fbank = torch.stack(fbanks, dim=0)\n",
        "        fbank = (fbank - fbank_mean) / (2 * fbank_std)\n",
        "        return fbank\n",
        "\n",
        "    def extract_labels(\n",
        "            self,\n",
        "            source: torch.Tensor,\n",
        "            padding_mask: Optional[torch.Tensor] = None,\n",
        "            fbank_mean: float = 15.41663,\n",
        "            fbank_std: float = 6.55582,\n",
        "    ):\n",
        "        fbank = self.preprocess(source, fbank_mean=fbank_mean, fbank_std=fbank_std)\n",
        "\n",
        "        print(1)\n",
        "\n",
        "        if padding_mask is not None:\n",
        "            padding_mask = self.forward_padding_mask(fbank, padding_mask)\n",
        "\n",
        "        fbank = fbank.unsqueeze(1)\n",
        "        features = self.patch_embedding(fbank)\n",
        "        features = features.reshape(features.shape[0], features.shape[1], -1)\n",
        "        features = features.transpose(1, 2)\n",
        "        features = self.layer_norm(features)\n",
        "\n",
        "        print(2)\n",
        "\n",
        "        if padding_mask is not None:\n",
        "            padding_mask = self.forward_padding_mask(features, padding_mask)\n",
        "\n",
        "        if self.post_extract_proj is not None:\n",
        "            features = self.post_extract_proj(features)\n",
        "\n",
        "        print(3)\n",
        "\n",
        "        x = self.dropout_input(features)\n",
        "\n",
        "        print(3.4)\n",
        "\n",
        "        x, layer_results = self.encoder(\n",
        "            x,\n",
        "            padding_mask=padding_mask,\n",
        "        )\n",
        "\n",
        "        print(4)\n",
        "\n",
        "        quantize_input = self.quantize_layer(x)\n",
        "        quantize_feature, embed_loss, embed_ind = self.quantize(quantize_input)\n",
        "\n",
        "        return embed_ind"
      ],
      "metadata": {
        "id": "LkXuGccx2noT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **History Prompt Setup**"
      ],
      "metadata": {
        "id": "MpjtPmNyRRBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cwE_adnRH7C",
        "outputId": "1613d336-b455-4849-9211-b4df56030d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/encodec/v0/encodec_24khz-d7cc33bc.th\" to /root/.cache/torch/hub/checkpoints/encodec_24khz-d7cc33bc.th\n",
            "100%|██████████| 88.9M/88.9M [00:01<00:00, 62.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchaudio.backend.sox_io_backend import load\n",
        "from encodec.model import EncodecModel, EncodedFrame\n",
        "from encodec.utils import convert_audio\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "config_dict: dict = torch.load('/content/gdrive/MyDrive/BEATS_tokenizer.pt')\n",
        "tokenizer_config: TokenizersConfig = TokenizersConfig(config_dict[\"cfg\"])\n",
        "\n",
        "AUDIO_ENCODER: EncodecModel = EncodecModel.encodec_model_24khz()\n",
        "AUDIO_TOKENIZER: Tokenizers = Tokenizers(tokenizer_config).to(\"cuda:0\")\n",
        "AUDIO_TOKENIZER.load_state_dict(config_dict[\"model\"])\n",
        "AUDIO_TOKENIZER.eval()\n",
        "\n",
        "def generate_history_prompt(audio_file_path: str, history_prompt_path: str, use_gpu: bool = True) -> int:\n",
        "\n",
        "    \"\"\"\n",
        "    Generates a history prompt for use with Bark and saves it to disk.\n",
        "\n",
        "    Args:\n",
        "\n",
        "        str audio_file_path: the path ot the audio file to use to make the prompt.\n",
        "\n",
        "        str history_prompt_path: the path to where to save the prompt to disk.\n",
        "\n",
        "        bool use_gpu: whether to use a GPU. Default: True\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        the sampling rate for the history prompt\n",
        "    \"\"\"\n",
        "\n",
        "    audio_tensor, sample_rate = load(audio_file_path)\n",
        "\n",
        "    fine_audio_tensor: torch.Tensor = get_preprocessed_audio(audio_file_path)\n",
        "\n",
        "    coarse_audio_tensor: torch.Tensor = fine_to_coarse(fine_audio_tensor)\n",
        "\n",
        "    print(\"HI\")\n",
        "\n",
        "    semantic_tokens = AUDIO_TOKENIZER.extract_labels(audio_tensor.to(\"cuda:0\"))\n",
        "\n",
        "    np.savez(history_prompt_path,\n",
        "        fine_prompt=fine_audio_tensor,\n",
        "        coarse_prompt=coarse_audio_tensor,\n",
        "        semantic_prompt=semantic_tokens,\n",
        "    )\n",
        "\n",
        "    return sample_rate\n",
        "\n",
        "def get_preprocessed_audio(audio_file_path: str) -> torch.Tensor:\n",
        "\n",
        "    \"\"\"\n",
        "    Retrieves an audio file and preprocesses it to a fine format.\n",
        "\n",
        "    Args:\n",
        "\n",
        "        str audio_file_path: the audio to preprocess.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        a PyTorch tensor representing the preprocessed audio.\n",
        "    \"\"\"\n",
        "\n",
        "    audio_tensor, sample_rate = load(audio_file_path)\n",
        "\n",
        "    audio_tensor = convert_audio(audio_tensor, sample_rate, AUDIO_ENCODER.sample_rate, 1).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        encoded_audio_frames: list[EncodedFrame] = AUDIO_ENCODER.encode(audio_tensor)\n",
        "\n",
        "    audio_codes: torch.Tensor = torch.cat([encoded_frame[0] for encoded_frame in encoded_audio_frames], dim=-1)\n",
        "\n",
        "    return audio_codes.squeeze()\n",
        "\n",
        "def fine_to_coarse(audio_tensor: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "    \"\"\"\n",
        "    Retrieves a coarse audio sample from a fine audio sample.\n",
        "    \"\"\"\n",
        "\n",
        "    return audio_tensor[:2, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **App Backend Logic**"
      ],
      "metadata": {
        "id": "liD-pG5sR4L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bark\n",
        "from bark import SAMPLE_RATE\n",
        "import torch\n",
        "from scipy.io.wavfile import write\n",
        "import os\n",
        "from bark.generation import ALLOWED_PROMPTS\n",
        "from torchaudio.backend.sox_io_backend import save\n",
        "\n",
        "def generate_speech(model_name: str, text: str, saved_audio_path: str) -> None:\n",
        "\n",
        "    \"\"\"\n",
        "    Converts a line of text to speech with a given model name and saves it as a waveform file.\n",
        "\n",
        "    Args:\n",
        "\n",
        "        str model: the name of the model to use for text-to-speech.\n",
        "\n",
        "        str text: the text to convert to speech.\n",
        "\n",
        "        str saved_audio_path: the path to save teh audio to.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    generated_audio = bark.generate_audio(text=text, history_prompt=model_name)\n",
        "\n",
        "    write(saved_audio_path, SAMPLE_RATE, generated_audio)"
      ],
      "metadata": {
        "id": "133nzPnVR89c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main App Script**"
      ],
      "metadata": {
        "id": "1_WFLSawb-bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "generate_history_prompt(\"/content/gdrive/MyDrive/voicedata/barackobamavoice.mp3\", \"/content/gdrive/MyDrive/barack_obama.npz\")\n",
        "\n",
        "generate_speech(\"v2/en_speaker_2\", \"Hi Donald [laughs]\", \"barack.wav\")"
      ],
      "metadata": {
        "id": "P3IHVIgUcBtL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "3d745bde-daa5-4dd9-db99-dc5c90f2e599"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HI\n",
            "1\n",
            "2\n",
            "3\n",
            "3.4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-745299f93560>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgenerate_history_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/voicedata/barackobamavoice.mp3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/gdrive/MyDrive/barack_obama.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgenerate_speech\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v2/en_speaker_2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Hi Donald [laughs]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"barack.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2e5ffe00a76f>\u001b[0m in \u001b[0;36mgenerate_history_prompt\u001b[0;34m(audio_file_path, history_prompt_path, use_gpu)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0msemantic_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAUDIO_TOKENIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     np.savez(history_prompt_path,\n",
            "\u001b[0;32m<ipython-input-5-6384fcc6a243>\u001b[0m in \u001b[0;36mextract_labels\u001b[0;34m(self, source, padding_mask, fbank_mean, fbank_std)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         x, layer_results = self.encoder(\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mpadding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3433ab7912ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, padding_mask, layer)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3433ab7912ab>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, x, padding_mask, tgt_layer)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mdropout_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdropout_probability\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attn_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtgt_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mlayer_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3433ab7912ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_weights, pos_bias)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             x, attn, pos_bias = self.self_attn(\n\u001b[0m\u001b[1;32m    242\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3433ab7912ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights, position_bias)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_relative_attention_bias\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mposition_bias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mposition_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0mposition_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3433ab7912ab>\u001b[0m in \u001b[0;36mcompute_bias\u001b[0;34m(self, query_length, key_length)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mmemory_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mrelative_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_position\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         relative_position_bucket = self._relative_positions_bucket(\n\u001b[0m\u001b[1;32m    416\u001b[0m             \u001b[0mrelative_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3433ab7912ab>\u001b[0m in \u001b[0;36m_relative_positions_bucket\u001b[0;34m(self, relative_positions, bidirectional)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mnum_buckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_buckets\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0mrelative_buckets\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrelative_positions\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_buckets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m             \u001b[0mrelative_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}